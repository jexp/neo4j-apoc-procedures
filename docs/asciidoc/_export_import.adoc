[[export-import]]
== Export / Import

=== Loading Data from Web-APIs

Supported protocols are `file`, `http`, `https`, `s3`, `hdfs` with redirect allowed. 

In case no protocol is passed, this procedure set will try to check whether the url is actually a file.

[NOTE]
As `apoc.import.file.use_neo4j_config` is enabled, the procedures check whether file system access is allowed and possibly constrained to a specific directory by
reading the two configuration parameters `dbms.security.allow_csv_import_from_file_urls` and `dbms.directories.import` respectively.
If you want to remove these constraints please set `apoc.import.file.use_neo4j_config=false`

[cols="1m,5"]
|===
| CALL apoc.load.json('http://example.com/map.json', [path], [config]) YIELD value as person CREATE (p:Person) SET p = person | load from JSON URL (e.g. web-api) to import JSON as stream of values if the JSON was an array or a single value if it was a map
| CALL apoc.load.xml('http://example.com/test.xml', ['xPath'], [config]) YIELD value as doc CREATE (p:Person) SET p.name = doc.name | load from XML URL (e.g. web-api) to import XML as single nested map with attributes and `+_type+`, `+_text+` and `+_children+` fields.
| CALL apoc.load.xmlSimple('http://example.com/test.xml') YIELD value as doc CREATE (p:Person) SET p.name = doc.name | load from XML URL (e.g. web-api) to import XML as single nested map with attributes and `+_type+`, `+_text+` fields and `+_<childtype>+` collections per child-element-type.
| CALL apoc.load.csv('url',{sep:";"}) YIELD lineNo, list, strings, map, stringMap | load CSV fom URL as stream of values +
config contains any of: `{skip:1,limit:5,header:false,sep:'TAB',ignore:['aColumn'],arraySep:';',results:['map','list','strings','stringMap'], +
nullValues:[''],mapping:{years:{type:'int',arraySep:'-',array:false,name:'age',ignore:false,nullValues:['n.A.']}}`
| CALL apoc.load.xls('url','Sheet'/'Sheet!A2:B5',{config}) YIELD lineNo, list, map | load XLS fom URL as stream of values +
config contains any of: `{skip:1,limit:5,header:false,ignore:['aColumn'],arraySep:';'+
nullValues:[''],mapping:{years:{type:'int',arraySep:'-',array:false,name:'age',ignore:false,nullValues:['n.A.']}}`
|===

include::loadjson.adoc[leveloffset=2]

include::loadcsv.adoc[leveloffset=+1]

include::loadxml.adoc[leveloffset=2]

include::loadhtml.adoc[leveloffset=2]

include::gephi.adoc[leveloffset=2]

==== Load Single File From Compressed File (zip/tar/tar.gz/tgz)

For load file from compressed file the url right syntax is:

----
apoc.load.csv("pathToFile!csv/fileName.csv")
----

----
apoc.load.json("https://github.com/neo4j-contrib/neo4j-apoc-procedures/tree/3.4/src/test/resources/testload.tgz?raw=true!person.json");
----

You have to put the ! character before the filename.

==== Using S3 protocol

For using S3 protocol you have to copy these jars into the plugins directory:

* aws-java-sdk-core-1.11.250.jar (https://mvnrepository.com/artifact/com.amazonaws/aws-java-sdk-core/1.11.250)
* aws-java-sdk-s3-1.11.250.jar (https://mvnrepository.com/artifact/com.amazonaws/aws-java-sdk-s3/1.11.250)
* httpclient-4.4.8.jar (https://mvnrepository.com/artifact/org.apache.httpcomponents/httpclient/4.5.4)
* httpcore-4.5.4.jar (https://mvnrepository.com/artifact/org.apache.httpcomponents/httpcore/4.4.8)
* joda-time-2.9.9.jar (https://mvnrepository.com/artifact/joda-time/joda-time/2.9.9)

S3 Url must be:

* s3://accessKey:secretKey@endpoint:port/bucket/key
or
* s3://endpoint:port/bucket/key?accessKey=accessKey&secretKey=secretKey


==== failOnError

Adding on config the parameter `failOnError:false` (by default `true`), in case of error the procedure don't fail but just return zero rows.

[[export-csv]]
=== Export to CSV

// tag::export.csv[]

`YIELD file, source, format, nodes, relationships, properties, time, rows, data`
[cols="1m,5"]
|===
| apoc.export.csv.query(query,file,config) | exports results from the Cypher statement as CSV to the provided file
| apoc.export.csv.all(file,config) | exports whole database as CSV to the provided file
| apoc.export.csv.data(nodes,rels,file,config) | exports given nodes and relationships as CSV to the provided file
| apoc.export.csv.graph(graph,file,config) | exports given graph object as CSV to the provided file
|===

If the file name is passed as `null` and the config `stream:true` the results are streamed back in the `data` column, e.g.

==== Note:

The labels exported are ordered alphabetically.
The output of `labels()` function is not sorted, use it in combination with `apoc.coll.sort()`.

[source,cypher]
----
CALL apoc.export.csv.all(null, {stream:true,batchSize:100}) YIELD data RETURN data
----

// end::export.csv[]

include::exportJson.adoc[leveloffset=1]

include::exportCypher.adoc[leveloffset=1]

[[graphml]]
=== GraphML Import / Export

GraphML is used by other tools, like Gephi and CytoScape to read graph data.

// tag::export.graphml[]

`YIELD file, source, format, nodes, relationships, properties, time`

[cols="1m,5"]
|===
| apoc.import.graphml(file-or-url,{batchSize: 10000, readLabels: true, storeNodeIds: false, defaultRelationshipType:"RELATED"}) | imports graphml into the graph
| apoc.export.graphml.all(file,config) | exports whole database as graphml to the provided file
| apoc.export.graphml.data(nodes,rels,file,config) | exports given nodes and relationships as graphml to the provided file
| apoc.export.graphml.graph(graph,file,config) | exports given graph object as graphml to the provided file
| apoc.export.graphml.query(query,file,config) | exports nodes and relationships from the Cypher statement as graphml to the provided file
|===

All `Point` or `Temporal` data types are exported formatted as a String

e.g:
[cols="1m,2"]
|===
|Point 3d | {"crs":"wgs-84-3d","latitude":56.7,"longitude":12.78,"height":100.0}
|Point 2d | {"crs":"wgs-84-3d","latitude":56.7,"longitude":12.78,"height":null}
|Date | 2018-10-10
|LocalDateTime | 2018-10-10T00:00
|===
// end::export.graphml[]

.configuration options
[options=header]
|===
| param | default | description
| format | | In export to Graphml script define the export format. Possible value is: "gephi"
| caption | | It's an array of string (i.e. ['name','title']) that define an ordered set of properties eligible as value for the `Label` value, if no match is found the there is a fallback to the node label, if the node label is missing the then the ID is used
| useTypes | false | Write the attribute type information to the graphml output
|===

==== Note:

The labels exported are ordered alphabetically.
The output of `labels()` function is not sorted, use it in combination with `apoc.coll.sort()`.

.configuration options
[options=header]
|===
| param | default | description
| batchSize | 20000 | define the batch size
// | silent | false | if enabled write progress output
| delim | "," | define the delimiter character (export csv)
| arrayDelim | ";" | define the delimiter character for arrays (used in the bulk import)
| quotes | 'always' | quote-character used for CSV, possible values are: 'always', 'none', 'ifNeeded'
| useTypes | false | add type on file header (export csv and graphml export)
| format | "neo4j-shell" | In export to Cypher script define the export format. Possible values are: "cypher-shell","neo4j-shell" and "plain"
| nodesOfRelationships | false | if enabled add relationship between nodes (export Cypher)
| storeNodeIds| false | set nodes' ids (import/export graphml)
| readLabels | false | read nodes' labels (import/export graphml)
| defaultRelationshipType | "RELATED" | set relationship type (import/export graphml)
| separateFiles | false | export results in separated file by type (nodes, relationships..)
| cypherFormat | create | In export to cypher script, define the cypher format (for example use `MERGE` instead of `CREATE`). Possible values are: "create", "updateAll", "addStructure", "updateStructure".
| bulkImport | true | In export it creates files for Neo4j Admin import
| separateHeader | false | In export it creates two file one for header and one for data
|===

Values for the `quotes` configuration:
* `none`: the same behaviour of the current `false`
* `always`:  the same behaviour of the current `true`
* `ifNeeded`: it applies quotes only when it's necessary;

When the config `bulkImport` is enable it create a list of file that can be used for Neo4j Bulk Import.

*This config can be used only with `apoc.export.csv.all` and `apoc.export.csv.graph`*

All file create are named as follow:

* Nodes file are construct with the name of the input file append with `.nodes.[LABEL_NAME].csv`
* Rel file are construct with the name of the input file append with `.relationships.[TYPE_NAME].csv`

If Node or Relationship have more than one Label/Type it will create one file for Label/Type.